{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15406033026645884796\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10164724876757140394\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16559829886605268262\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch: <_VariantDataset shapes: (), types: tf.int32>\n",
      "X_batch: <_VariantDataset shapes: (), types: tf.int32>\n",
      "X_batch: <_VariantDataset shapes: (), types: tf.int32>\n",
      "X_batch: <_VariantDataset shapes: (), types: tf.int32>\n",
      "X_batch: <_VariantDataset shapes: (), types: tf.int32>\n",
      "X_batch: <_VariantDataset shapes: (), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "n_steps=5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(16))\n",
    "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "for idx, (X_batch) in enumerate(dataset):\n",
    "    print('X_batch:',X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch: tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int32)\n",
      "X_batch: tf.Tensor([2 3 4 5 6], shape=(5,), dtype=int32)\n",
      "X_batch: tf.Tensor([4 5 6 7 8], shape=(5,), dtype=int32)\n",
      "X_batch: tf.Tensor([ 6  7  8  9 10], shape=(5,), dtype=int32)\n",
      "X_batch: tf.Tensor([ 8  9 10 11 12], shape=(5,), dtype=int32)\n",
      "X_batch: tf.Tensor([10 11 12 13 14], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "for idx, (X_batch) in enumerate(dataset):\n",
    "    print('X_batch:',X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch: tf.Tensor(\n",
      "[[10 11 12 13]\n",
      " [ 2  3  4  5]\n",
      " [ 8  9 10 11]], shape=(3, 4), dtype=int32)\n",
      "y_batch: tf.Tensor(\n",
      "[[11 12 13 14]\n",
      " [ 3  4  5  6]\n",
      " [ 9 10 11 12]], shape=(3, 4), dtype=int32)\n",
      "X_batch: tf.Tensor(\n",
      "[[0 1 2 3]\n",
      " [4 5 6 7]\n",
      " [6 7 8 9]], shape=(3, 4), dtype=int32)\n",
      "y_batch: tf.Tensor(\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 7  8  9 10]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(1)\n",
    "for idx, (X_batch, y_batch) in enumerate(dataset):\n",
    "    print('X_batch:',X_batch)\n",
    "    print('y_batch:',y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CharRNN - Shakespeare_Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "1122304/1115394 [==============================] - 15s 13us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\.keras\\\\datasets\\\\shakespeare.txt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath) as f:\n",
    "    shakespeare_txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to \n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_txt[:140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(sorted(set(shakespeare_txt.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['first'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 1,\n",
       " 'e': 2,\n",
       " 't': 3,\n",
       " 'o': 4,\n",
       " 'a': 5,\n",
       " 'i': 6,\n",
       " 'h': 7,\n",
       " 's': 8,\n",
       " 'r': 9,\n",
       " 'n': 10,\n",
       " '\\n': 11,\n",
       " 'l': 12,\n",
       " 'd': 13,\n",
       " 'u': 14,\n",
       " 'm': 15,\n",
       " 'y': 16,\n",
       " 'w': 17,\n",
       " ',': 18,\n",
       " 'c': 19,\n",
       " 'f': 20,\n",
       " 'g': 21,\n",
       " 'b': 22,\n",
       " 'p': 23,\n",
       " ':': 24,\n",
       " 'k': 25,\n",
       " 'v': 26,\n",
       " '.': 27,\n",
       " \"'\": 28,\n",
       " ';': 29,\n",
       " '?': 30,\n",
       " '!': 31,\n",
       " '-': 32,\n",
       " 'j': 33,\n",
       " 'q': 34,\n",
       " 'x': 35,\n",
       " 'z': 36,\n",
       " '3': 37,\n",
       " '&': 38,\n",
       " '$': 39}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = tokenizer.word_index\n",
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = tokenizer.document_count\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1,\n",
       "       19,  3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,\n",
       "        9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8,  0,\n",
       "       14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11, 23, 10,  7,\n",
       "       22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10, 10, 19,  5,  8,\n",
       "        7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 15,  3, 13])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_txt]))-1\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167309\n"
     ]
    }
   ],
   "source": [
    "train_size = dataset_size*15//100\n",
    "print(train_size)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x in dataset.take(2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=100\n",
    "windows_length=n_steps+1\n",
    "batch_size=32\n",
    "dataset = dataset.repeat().window(windows_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(windows_length))\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[1:])).shuffle(10000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_batch: tf.Tensor(\n",
      "[[ 8  5 18 ... 14  5  2]\n",
      " [ 2  6  1 ... 12  5 12]\n",
      " [ 1  1  0 ...  1  0 19]\n",
      " ...\n",
      " [12 26  0 ... 13  0  4]\n",
      " [15  0 25 ...  3  0 21]\n",
      " [ 6  1  0 ...  1 11 22]], shape=(32, 100), dtype=int32)\n",
      "Y_batch: tf.Tensor(\n",
      "[[ 5 18  5 ...  5  2 15]\n",
      " [ 6  1  0 ...  5 12  0]\n",
      " [ 1  0 16 ...  0 19 11]\n",
      " ...\n",
      " [26  0  6 ...  0  4  8]\n",
      " [ 0 25  1 ...  0 21  8]\n",
      " [ 1  0  4 ... 11 22  7]], shape=(32, 100), dtype=int32)\n",
      "\n",
      "X_batch: tf.Tensor(\n",
      "[[ 3 16  0 ... 12  0  1]\n",
      " [ 5  9 24 ...  5 14  1]\n",
      " [ 2  5 35 ...  5  9  0]\n",
      " ...\n",
      " [ 7  1  9 ... 12  0  2]\n",
      " [ 0  1  4 ...  1  7  7]\n",
      " [ 0  5 14 ...  8 14  7]], shape=(32, 100), dtype=int32)\n",
      "Y_batch: tf.Tensor(\n",
      "[[16  0  5 ...  0  1  7]\n",
      " [ 9 24  0 ... 14  1  0]\n",
      " [ 5 35  1 ...  9  0  8]\n",
      " ...\n",
      " [ 1  9  4 ...  0  2  6]\n",
      " [ 1  4  2 ...  7  7  0]\n",
      " [ 5 14 22 ... 14  7 17]], shape=(32, 100), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset.take(2):\n",
    "    print('\\nX_batch:',x)\n",
    "    print('Y_batch:',y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, y_batch: (tf.one_hot(X_batch, depth=max_id), y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_batch: tf.Tensor(\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]], shape=(32, 100, 39), dtype=float32)\n",
      "Y_batch: tf.Tensor(\n",
      "[[ 0  8  1 ... 12  4  5]\n",
      " [16  6  3 ...  0  5  9]\n",
      " [15 17  0 ... 15  0  6]\n",
      " ...\n",
      " [23 10  5 ...  8  0  2]\n",
      " [10 16  3 ...  9 12  7]\n",
      " [12  4  9 ...  1  8  0]], shape=(32, 100), dtype=int32)\n",
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset.take(1):\n",
    "    print('\\nX_batch:',x)\n",
    "    print('Y_batch:',y)\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_9 (GRU)                  (None, None, 64)          20160     \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, None, 39)          2535      \n",
      "=================================================================\n",
      "Total params: 22,695\n",
      "Trainable params: 22,695\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(64, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 5228 steps\n",
      "Epoch 1/2\n",
      "5228/5228 [==============================] - 184s 35ms/step - loss: 2.0332\n",
      "Epoch 2/2\n",
      "5228/5228 [==============================] - 170s 33ms/step - loss: 1.8139\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
    "                    epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 13,  0,  2,  9,  1,  0,  2,  3, 13]], dtype=int64)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "next_char(\"How are yo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tycc ba\n",
      "doveay enerobpance, cra\n",
      "vijoevins! drazeryp\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(windows_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(windows_length))\n",
    "dataset = dataset.repeat().batch(1)\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.map(lambda X_batch, y_batch:\n",
    "                                         (tf.one_hot(X_batch, depth=max_id), y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(windows_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(windows_length))\n",
    "    datasets.append(dataset)\n",
    "\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(64, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_16 (GRU)                 (32, None, 128)           64896     \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (32, None, 64)            37248     \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, None, 39)          2535      \n",
      "=================================================================\n",
      "Total params: 104,679\n",
      "Trainable params: 104,679\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps\n",
      "Epoch 1/7\n",
      "52/52 [==============================] - 48s 924ms/step - loss: 3.6636\n",
      "Epoch 2/7\n",
      "52/52 [==============================] - 40s 772ms/step - loss: 3.6636\n",
      "Epoch 3/7\n",
      "52/52 [==============================] - 39s 750ms/step - loss: 3.6636\n",
      "Epoch 4/7\n",
      "52/52 [==============================] - 38s 735ms/step - loss: 3.6636\n",
      "Epoch 5/7\n",
      "52/52 [==============================] - 39s 755ms/step - loss: 3.6636\n",
      "Epoch 6/7\n",
      "52/52 [==============================] - 38s 730ms/step - loss: 3.6636\n",
      "Epoch 7/7\n",
      "52/52 [==============================] - 38s 729ms/step - loss: 3.6636\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "    steps_per_epoch = train_size // batch_size // n_steps   # Becoz shift=m_steps (train_size/(n_steps*batch_size))\n",
    "    history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=7,\n",
    "                        callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(64, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 121s 7us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_test), (X_valid, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 36s 22us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<sos> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx = keras.datasets.imdb.get_word_index()\n",
    "id2word = {id_+ 3 : word for word, id_ in word_idx.items()}\n",
    "for id_, word in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "    id2word[id_]=word\n",
    "\n",
    "' '.join([id2word[id_] for id_ in X_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to C:\\Users\\user\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9f4fbae8fa4c388741524602db35da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4aadf6c9b19485797588d8a484c728a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\user\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteHI5H61\\imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0c7ea88e5049c29a77f056fdeffa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\user\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteHI5H61\\imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b483b0feede4ce2af6d08a3e9e1f3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\user\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteHI5H61\\imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf581f3a1df4188ad70d70a12902136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\user\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'unsupervised'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = info.splits['train'].num_examples\n",
    "test_size = info.splits['test'].num_examples\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  This is a big step down after the surprisingly enjoyable original. This sequel isn't nearly as fun as part one, and it instead spends too much time on plot development. Tim Thomerson is still the best thing about this series, but his wisecracking is  .....\n",
      "Label:  0 \n",
      "\n",
      "Review:  Perhaps because I was so young, innocent and BRAINWASHED when I saw it, this movie was the cause of many sleepless nights for me. I haven't seen it since I was in seventh grade at a Presbyterian school, so I am not sure what effect it would have on m .....\n",
      "Label:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets['train'].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print('Review: ', review.decode('utf-8')[:250],'.....')\n",
    "        print('Label: ', label, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b'<br\\s*/?>', b' ')\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b'[^a-zA-Z]', b' ')\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value='<pad>'), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=394, shape=(2, 61), dtype=string, numpy=\n",
       " array([[b'This', b'is', b'a', b'big', b'step', b'down', b'after', b'the',\n",
       "         b'surprisingly', b'enjoyable', b'original', b'This', b'sequel',\n",
       "         b'isn', b't', b'nearly', b'as', b'fun', b'as', b'part', b'one',\n",
       "         b'and', b'it', b'instead', b'spends', b'too', b'much', b'time',\n",
       "         b'on', b'plot', b'development', b'Tim', b'Thomerson', b'is',\n",
       "         b'still', b'the', b'best', b'thing', b'about', b'this',\n",
       "         b'series', b'but', b'his', b'wisecracking', b'is', b'toned',\n",
       "         b'down', b'in', b'this', b'entry', b'The', b'performances',\n",
       "         b'are', b'all', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "         b'<pad>', b'<pad>'],\n",
       "        [b'Perhaps', b'because', b'I', b'was', b'so', b'young',\n",
       "         b'innocent', b'and', b'BRAINWASHED', b'when', b'I', b'saw',\n",
       "         b'it', b'this', b'movie', b'was', b'the', b'cause', b'of',\n",
       "         b'many', b'sleepless', b'nights', b'for', b'me', b'I', b'haven',\n",
       "         b't', b'seen', b'it', b'since', b'I', b'was', b'in', b'seventh',\n",
       "         b'grade', b'at', b'a', b'Presbyterian', b'school', b'so', b'I',\n",
       "         b'am', b'not', b'sure', b'what', b'effect', b'it', b'would',\n",
       "         b'have', b'on', b'me', b'now', b'However', b'I', b'will', b'say',\n",
       "         b'that', b'it', b'left', b'an', b'impress']], dtype=object)>,\n",
       " <tf.Tensor: id=213, shape=(2,), dtype=int64, numpy=array([0, 0], dtype=int64)>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets['train'].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 223998),\n",
       " (b'the', 61156),\n",
       " (b'a', 38569),\n",
       " (b'of', 33984),\n",
       " (b'and', 33432)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49739"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([b'<pad>', b'the', b'a', b'of', b'and'], 10000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "truncated_vocabulary[:5], len(truncated_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {word: idx for idx, word in enumerate(truncated_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\t12\t108\t105\t10000\t"
     ]
    }
   ],
   "source": [
    "for word in b\"This movie we do itt\".split():\n",
    "    print(word_to_id.get(word) or vocab_size, end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "oov_buckets=1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=102106, shape=(1, 4), dtype=int64, numpy=array([[   24,    12,    13, 10053]], dtype=int64)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].repeat().batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   24     7     2 ...     0     0     0]\n",
      " [ 1230    83     5 ...    31  4258     0]\n",
      " [ 4112     3     1 ...     0     0     0]\n",
      " ...\n",
      " [   24     7    25 ...     0     0     0]\n",
      " [ 1292  3544     7 ...     0     0     0]\n",
      " [10928 10687  4552 ...     0     0     0]], shape=(32, 62), dtype=int64)\n",
      "tf.Tensor([0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = datasets['test'].take(2000).repeat().batch(32).map(preprocess)\n",
    "valid_set = valid_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  29 1794   17 ...    0    0    0]\n",
      " [   5  264  254 ...    0    0    0]\n",
      " [  24  323   16 ...    0    0    0]\n",
      " ...\n",
      " [  15   95  167 ...    0    0    0]\n",
      " [ 159    7  340 ...    0    0    0]\n",
      " [   5  100   10 ...    2  105    0]], shape=(32, 62), dtype=int64) tf.Tensor([1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for x, y in valid_set.take(1):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 128)         1408000   \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, None, 64)          37248     \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 32)                9408      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,454,689\n",
      "Trainable params: 1,454,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "# mask_zero=True means word that mapped as 0 is gonna get neglect in its time step and previous hidden state would be moved\n",
    "    keras.layers.Embedding(vocab_size + oov_buckets, embed_size, mask_zero=True, input_shape=[None]),\n",
    "    keras.layers.GRU(64, return_sequences=True),\n",
    "    keras.layers.GRU(32),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "root_dir = os.path.join(os.curdir, 'my_logs')\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(root_dir)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 781 steps, validate for 62 steps\n",
      "Epoch 1/5\n",
      "781/781 [==============================] - 117s 150ms/step - loss: 0.5327 - accuracy: 0.7284 - val_loss: 0.4999 - val_accuracy: 0.7591\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 74s 95ms/step - loss: 0.3502 - accuracy: 0.8581 - val_loss: 0.5479 - val_accuracy: 0.7480\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 70s 89ms/step - loss: 0.2095 - accuracy: 0.9255 - val_loss: 0.6583 - val_accuracy: 0.7329\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 66s 85ms/step - loss: 0.1521 - accuracy: 0.9461 - val_loss: 0.7532 - val_accuracy: 0.7349\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 61s 78ms/step - loss: 0.0965 - accuracy: 0.9683 - val_loss: 1.0095 - val_accuracy: 0.7389\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, steps_per_epoch = train_size//32,\n",
    "                    validation_data=valid_set, validation_steps=2000//32,\n",
    "                    epochs=5, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, None, 128)    1408000     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, None)         0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_14 (GRU)                    (None, None, 64)     37248       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_15 (GRU)                    (None, 32)           9408        gru_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            33          gru_15[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,454,689\n",
      "Trainable params: 1,454,689\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K = keras.backend\n",
    "embed_size = 128\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(64, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(32)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 781 steps, validate for 62 steps\n",
      "Epoch 1/3\n",
      "781/781 [==============================] - 92s 118ms/step - loss: 0.5313 - accuracy: 0.7242 - val_loss: 0.4972 - val_accuracy: 0.7641\n",
      "Epoch 2/3\n",
      "781/781 [==============================] - 46s 58ms/step - loss: 0.3498 - accuracy: 0.8550 - val_loss: 0.5538 - val_accuracy: 0.7485\n",
      "Epoch 3/3\n",
      "781/781 [==============================] - 36s 47ms/step - loss: 0.2054 - accuracy: 0.9260 - val_loss: 0.6493 - val_accuracy: 0.7243\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, steps_per_epoch = train_size//32,\n",
    "                    validation_data=valid_set, validation_steps=2000//32,\n",
    "                    epochs=3, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, 'my_tfhub_cache')\n",
    "os.environ['TFHUB_CACHE_DIR'] = TFHUB_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_5 (KerasLayer)   (None, 50)                48190600  \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               6528      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 48,197,257\n",
      "Trainable params: 6,657\n",
      "Non-trainable params: 48,190,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # 1 at the end means version 1\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\saved_model.pb\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\assets\\tokens.txt\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\variables\\variables.data-00000-of-00001\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\variables\\variables.index\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirname, filenames in os.walk(TFHUB_CACHE_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 781 steps, validate for 62 steps\n",
      "Epoch 1/5\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5484 - accuracy: 0.7233 - val_loss: 0.5333 - val_accuracy: 0.7384\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5146 - accuracy: 0.7490 - val_loss: 0.5265 - val_accuracy: 0.7450\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5091 - accuracy: 0.7520 - val_loss: 0.5238 - val_accuracy: 0.7475\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 25s 31ms/step - loss: 0.5054 - accuracy: 0.7554 - val_loss: 0.5204 - val_accuracy: 0.7510\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5022 - accuracy: 0.7562 - val_loss: 0.5168 - val_accuracy: 0.7550\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].repeat().batch(batch_size).prefetch(1)\n",
    "valid_set = datasets['test'].take(2000).repeat().batch(batch_size).prefetch(1)\n",
    "\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size,\n",
    "                    validation_data=valid_set, validation_steps=2000//batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "embed_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(128, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(128)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
    "\n",
    "f_output, f_state, f_seq_len = decoder(decoder_embeddings, training=True,\n",
    "                                       initial_state=encoder_state, sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(f_output.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths], \n",
    "                          outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 2s 66ms/step - loss: 4.6053\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 4.6037\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
    "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
    "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
    "seq_lengths = np.full([1000], 15)\n",
    "\n",
    "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, None, 10)          660       \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 20)          1320      \n",
      "=================================================================\n",
      "Total params: 1,980\n",
      "Trainable params: 1,980\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_dims, max_steps, dtype=tf.float32, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims%2 ==1: max_dims+=1\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims//2))\n",
    "        pos = np.empty((1, max_steps, max_dims))\n",
    "        pos[0, :, ::2] = np.sin(p/(10000**(2*i/max_dims))).T\n",
    "        pos[0, :, 1::2] = np.cos(p/(10000**(2*i/max_dims))).T\n",
    "        self.position_embed = tf.constant(pos.astype(dtype=self.dtype))\n",
    "    def call(self, inputs):\n",
    "        shape=tf.shape(inputs)\n",
    "        return inputs + self.position_embed[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 201\n",
    "max_dims = 512\n",
    "pos_emb = PositionalEncoding(max_dims, max_steps)\n",
    "PE = pos_emb(np.zeros((1, max_steps, max_dims), np.float32))[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i1, i2, crop_i = 100, 101, 150\n",
    "p1, p2, p3 = 22, 60, 35\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\n",
    "plt.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\n",
    "plt.plot(p3, PE[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\n",
    "plt.plot(PE[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\n",
    "plt.plot(PE[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\n",
    "plt.plot([p1, p2], [PE[p1, i1], PE[p2, i1]], \"bo\")\n",
    "plt.plot([p1, p2], [PE[p1, i2], PE[p2, i2]], \"ro\")\n",
    "plt.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\n",
    "plt.xlim((0, 201))\n",
    "plt.ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.hlines(0, 0, max_steps - 1, color=\"k\", linewidth=1, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512; max_steps = 500; vocab_size = 10000\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "positional_encoding = PositionalEncoding(max_dims=embed_size, max_steps=max_steps)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = encoder_in\n",
    "for N in range(6):\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "encoder_outputs = Z\n",
    "Z = decoder_in\n",
    "for N in range(6):\n",
    "    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
    "\n",
    "outputs = keras.layers.TimeDistributed(\n",
    "                            keras.layers.Dense(vocab_size, activation='softmax'))(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs], \n",
    "                          outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 512)    5120000     input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "positional_encoding_7 (Position (None, None, 512)    0           embedding_3[0][0]                \n",
      "                                                                 embedding_3[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_36 (Attention)        (None, None, 512)    1           positional_encoding_7[0][0]      \n",
      "                                                                 positional_encoding_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_37 (Attention)        (None, None, 512)    1           attention_36[0][0]               \n",
      "                                                                 attention_36[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_38 (Attention)        (None, None, 512)    1           attention_37[0][0]               \n",
      "                                                                 attention_37[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_39 (Attention)        (None, None, 512)    1           attention_38[0][0]               \n",
      "                                                                 attention_38[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_40 (Attention)        (None, None, 512)    1           attention_39[0][0]               \n",
      "                                                                 attention_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_42 (Attention)        (None, None, 512)    1           positional_encoding_7[1][0]      \n",
      "                                                                 positional_encoding_7[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_41 (Attention)        (None, None, 512)    1           attention_40[0][0]               \n",
      "                                                                 attention_40[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_43 (Attention)        (None, None, 512)    1           attention_42[0][0]               \n",
      "                                                                 attention_41[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_44 (Attention)        (None, None, 512)    1           attention_43[0][0]               \n",
      "                                                                 attention_43[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_45 (Attention)        (None, None, 512)    1           attention_44[0][0]               \n",
      "                                                                 attention_41[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_46 (Attention)        (None, None, 512)    1           attention_45[0][0]               \n",
      "                                                                 attention_45[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_47 (Attention)        (None, None, 512)    1           attention_46[0][0]               \n",
      "                                                                 attention_41[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_48 (Attention)        (None, None, 512)    1           attention_47[0][0]               \n",
      "                                                                 attention_47[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_49 (Attention)        (None, None, 512)    1           attention_48[0][0]               \n",
      "                                                                 attention_41[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_50 (Attention)        (None, None, 512)    1           attention_49[0][0]               \n",
      "                                                                 attention_49[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_51 (Attention)        (None, None, 512)    1           attention_50[0][0]               \n",
      "                                                                 attention_41[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_52 (Attention)        (None, None, 512)    1           attention_51[0][0]               \n",
      "                                                                 attention_51[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_53 (Attention)        (None, None, 512)    1           attention_52[0][0]               \n",
      "                                                                 attention_41[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 10000)  5130000     attention_53[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,250,018\n",
      "Trainable params: 10,250,018\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Format Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# cannot use strftime()'s %B format since it depends on the locale\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "    ordinals = np.random.randint(max_date-min_date, size=n_dates)+min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "    X = [MONTHS[dt.month - 1] + ' ' + dt.strftime('%d, %Y') for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "February 17, 3118        3118-02-17               \n",
      "October 12, 1760         1760-10-12               \n",
      "September 29, 1076       1076-09-29               \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
    "print(\"-\" * 50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADFJMNOSabceghilmnoprstuvy01234567890, '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS)))) + \"01234567890, \"\n",
    "INPUT_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHARS = \"0123456789-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS.index('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_str_to_ids(dt_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(d) for d in dt_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 11, 9, 20, 23, 8, 20, 25, 38, 27, 33, 37, 38, 29, 27, 27, 34]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(x_example[0], INPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'February 17, 3118'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_example[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 1, 8, 10, 0, 2, 10, 1, 7]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3118-02-17'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_example[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[1, 2, 3, 4], [1, 2], [1, 3, 5], [4]], [[1], [2, 5], [1, 3, 5, 3]]]>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ragged.constant([[[1,2,3,4],[1,2], [1,3, 5], [4]],[[1], [2,5], [1,3,5,3]]], ragged_rank=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_str, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_str]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X+1).to_tensor() # using 0 as the padding token\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    X,y = random_dates(n_dates)\n",
    "    return prepare_date_strs(X, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "X_train, Y_train = create_dataset(10000)\n",
    "X_valid, Y_valid = create_dataset(2000)\n",
    "X_test, Y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_6 (Sequential)    (None, 128)               83712     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (None, 10, 12)            133132    \n",
      "=================================================================\n",
      "Total params: 216,844\n",
      "Trainable params: 216,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = Y_train.shape[1]\n",
    "\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim = len(INPUT_CHARS)+1, \n",
    "                           output_dim=embedding_size, input_shape=[None]),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS)+1, activation='softmax')\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(max_output_length), # Repeat the output of the encoder to the decoder \n",
    "    decoder\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 26s 81ms/step - loss: 1.8369 - accuracy: 0.3407 - val_loss: 1.5001 - val_accuracy: 0.4459\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 1.3002 - accuracy: 0.5215 - val_loss: 1.1315 - val_accuracy: 0.5938\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 1.1204 - accuracy: 0.6066 - val_loss: 0.8853 - val_accuracy: 0.6777\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 21s 67ms/step - loss: 0.7481 - accuracy: 0.7182 - val_loss: 0.6499 - val_accuracy: 0.7481\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.9049 - accuracy: 0.6725 - val_loss: 0.6976 - val_accuracy: 0.7385\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.5255 - accuracy: 0.7983 - val_loss: 0.4068 - val_accuracy: 0.8457\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.4293 - accuracy: 0.8497 - val_loss: 0.2948 - val_accuracy: 0.8932\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.2247 - accuracy: 0.9283 - val_loss: 0.3782 - val_accuracy: 0.8652\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.1346 - accuracy: 0.9663 - val_loss: 0.1012 - val_accuracy: 0.9769\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.2155 - accuracy: 0.9429 - val_loss: 1.3195 - val_accuracy: 0.5037\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.2869 - accuracy: 0.9189 - val_loss: 0.0808 - val_accuracy: 0.9891\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.0546 - accuracy: 0.9941 - val_loss: 0.0419 - val_accuracy: 0.9955\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 20s 64ms/step - loss: 0.0303 - accuracy: 0.9976 - val_loss: 0.0265 - val_accuracy: 0.9979\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.0194 - accuracy: 0.9989 - val_loss: 0.0178 - val_accuracy: 0.9989\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 25s 81ms/step - loss: 0.0133 - accuracy: 0.9996 - val_loss: 0.0128 - val_accuracy: 0.9994\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 26s 81ms/step - loss: 0.0095 - accuracy: 0.9998 - val_loss: 0.0098 - val_accuracy: 0.9997\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 21s 67ms/step - loss: 0.0069 - accuracy: 0.9999 - val_loss: 0.0073 - val_accuracy: 0.9998\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 26s 82ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 0.9998\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 0.9998\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                    validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return[''.join([('?' + chars)[index] for index in seq])\n",
    "                                      for seq in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 18), dtype=int32, numpy=\n",
       "array([[ 8, 12, 20, 23, 12, 17, 10, 12, 21, 39, 28, 34, 38, 39, 29, 27,\n",
       "        27, 36],\n",
       "       [ 4, 24, 16, 26, 39, 28, 31, 38, 39, 28, 34, 35, 36,  0,  0,  0,\n",
       "         0,  0]])>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict_classes(X_new)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was trained on the larger sequence i.e. 18 so it won't work best for shorter sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 13), dtype=int32, numpy=array([[ 4, 24, 16, 26, 39, 28, 31, 38, 39, 28, 34, 35, 36]])>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789-09-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict_classes(X_new)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have to use padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs_padded(date_str):\n",
    "    X = prepare_date_strs(date_str)\n",
    "    if X.shape[1]<max_input_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs_padded([\"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 18), dtype=int32, numpy=\n",
       "array([[ 4, 24, 16, 26, 39, 28, 31, 38, 39, 28, 34, 35, 36,  0,  0,  0,\n",
       "         0,  0]])>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "ids = model.predict_classes(X_new)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Feeding (Feeding shiftwd Target to the decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS)+1\n",
    "\n",
    "def shifted_output_sequences(Y):\n",
    "    sos_tokens = tf.fill(dims=(len(Y),1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_decoder = shifted_output_sequences(Y_train)\n",
    "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
    "X_test_decoder = shifted_output_sequences(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
       "array([[12,  4,  2, ...,  3, 11,  2],\n",
       "       [12,  2,  8, ...,  1, 11,  2],\n",
       "       [12,  2,  1, ..., 10, 11,  3],\n",
       "       ...,\n",
       "       [12,  9,  9, ...,  8, 11,  1],\n",
       "       [12,  2,  4, ...,  3, 11,  1],\n",
       "       [12,  2,  9, ...,  2, 11,  1]])>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, None, 32)     1280        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, None, 32)     416         input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  [(None, 128), (None, 82432       embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, None, 128)    82432       embedding_15[0][0]               \n",
      "                                                                 lstm_13[0][1]                    \n",
      "                                                                 lstm_13[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 12)     1548        lstm_14[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 168,108\n",
      "Trainable params: 168,108\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = keras.layers.Embedding(input_dim = len(INPUT_CHARS)+1, \n",
    "                                           output_dim=encoder_embedding_size)(encoder_inputs)\n",
    "_, encoder_h, encoder_c = keras.layers.LSTM(lstm_units, return_state=True)(encoder_embedding)\n",
    "encoder_state=[encoder_h, encoder_c]\n",
    "\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = keras.layers.Embedding(input_dim = len(OUTPUT_CHARS)+2, \n",
    "                                           output_dim=decoder_embedding_size)(decoder_inputs)\n",
    "decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(decoder_embedding, initial_state=encoder_state)\n",
    "decoder_output = keras.layers.Dense(len(OUTPUT_CHARS)+1, activation='softmax')(decoder_lstm_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                                      outputs=[decoder_output])\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 1.6606 - accuracy: 0.3757 - val_loss: 1.4033 - val_accuracy: 0.4552\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 1.1996 - accuracy: 0.5422 - val_loss: 0.9914 - val_accuracy: 0.6321\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 0.6927 - accuracy: 0.7556 - val_loss: 0.4006 - val_accuracy: 0.8805\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.2480 - accuracy: 0.9347 - val_loss: 0.1320 - val_accuracy: 0.9755\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.0975 - accuracy: 0.9854 - val_loss: 0.0464 - val_accuracy: 0.9979\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.0552 - accuracy: 0.9940 - val_loss: 0.0300 - val_accuracy: 0.9991\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 23s 75ms/step - loss: 0.0191 - accuracy: 0.9999 - val_loss: 0.0158 - val_accuracy: 0.9999\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.0488 - accuracy: 0.9915 - val_loss: 0.0153 - val_accuracy: 0.9998\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 0.9999\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS)+1\n",
    "\n",
    "def predict_date_strs(date_str):\n",
    "    X = prepare_date_strs_padded(date_str)\n",
    "    y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length-y_pred.shape[0]\n",
    "        X_decoder = tf.pad(y_pred, [[0,0], [0, pad_size]])\n",
    "        y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
    "        y_pred_next = tf.argmax(y_probas_next, axis=-1, output_type = tf.int32)\n",
    "        y_pred = tf.concat([y_pred, y_pred_next], axis=1)\n",
    "    return ids_to_date_strs(y_pred[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 12)\n",
      "(2, 1, 12)\n",
      "(2, 1, 12)\n",
      "(2, 1, 12)\n",
      "(2, 1, 12)\n",
      "(2, 1, 12)\n",
      "(2, 1, 12)\n",
      "(2, 1, 12)\n",
      "(2, 1, 12)\n",
      "(2, 1, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_Addons Basic seq2seq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, None, 32)     1280        input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, None, 32)     1312        input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  [(None, 128), (None, 82432       embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "basic_decoder_1 (BasicDecoder)  (BasicDecoderOutput( 83980       embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 12)     0           basic_decoder_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 169,004\n",
      "Trainable params: 169,004\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
    "                                                 sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, training=True,\n",
    "    initial_state=encoder_state)\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
    "\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                           outputs=[Y_proba])\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 1.6739 - accuracy: 0.3656 - val_loss: 1.4868 - val_accuracy: 0.4063\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 1.3953 - accuracy: 0.4562 - val_loss: 1.3563 - val_accuracy: 0.4618\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 23s 73ms/step - loss: 1.0973 - accuracy: 0.5914 - val_loss: 0.8483 - val_accuracy: 0.7071\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 0.6123 - accuracy: 0.7896 - val_loss: 0.3597 - val_accuracy: 0.8931\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 18s 58ms/step - loss: 0.2094 - accuracy: 0.9518 - val_loss: 0.1155 - val_accuracy: 0.9818\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.0903 - accuracy: 0.9885 - val_loss: 0.0467 - val_accuracy: 0.9972\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.0327 - accuracy: 0.9993 - val_loss: 0.0260 - val_accuracy: 0.9994\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.0375 - accuracy: 0.9952 - val_loss: 0.0180 - val_accuracy: 0.9998\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.0131 - accuracy: 0.9999 - val_loss: 0.0111 - val_accuracy: 0.9999\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 0.9999\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 0.0551 - accuracy: 0.9894 - val_loss: 0.0071 - val_accuracy: 0.9999\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 20s 63ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=15,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To automatically provide the last predicted class to the next time step during inference\n",
    "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "    embedding_fn=decoder_embedding_layer)\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
    "    maximum_iterations=max_output_length) # maximum_iterations are necessary otherwise it would run on an infinite loop\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=sos_id) # start token is also important\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens, training=True,\n",
    "    initial_state=encoder_state,\n",
    "    start_tokens=start_tokens,\n",
    "    end_token=0)\n",
    "\n",
    "inference_model = keras.models.Model(inputs=[encoder_inputs],\n",
    "                                     outputs=[final_outputs.sample_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Seq2Seq ScheduledSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 32)     1280        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 32)     1312        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 128), (None, 82432       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "basic_decoder_1 (BasicDecoder)  (BasicDecoderOutput( 83980       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 12)     0           basic_decoder_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 169,004\n",
      "Trainable params: 169,004\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "n_epochs = 20\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.ScheduledEmbeddingTrainingSampler(sampling_probability=0,\n",
    "                                                                embedding_fn=decoder_embedding_layer)\n",
    "# we must set the sampling_probability after creating the sampler\n",
    "sampler.sampling_probability = tf.Variable(0.)\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
    "                                                 sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, training=True,\n",
    "    initial_state=encoder_state)\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                           outputs=[Y_proba])\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sampling_probability(epochs, logs):\n",
    "    proba = min(1.0, epochs/(n_epochs-1))\n",
    "    sampler.sampling_probability.assign(proba)\n",
    "\n",
    "sampling_probability_cb = keras.callbacks.LambdaCallback(on_epoch_begin=update_sampling_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 26s 84ms/step - loss: 1.6641 - accuracy: 0.3771 - val_loss: 1.3807 - val_accuracy: 0.4690\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 1.1808 - accuracy: 0.5513 - val_loss: 0.9317 - val_accuracy: 0.6484\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 10s 33ms/step - loss: 0.7171 - accuracy: 0.7403 - val_loss: 0.5232 - val_accuracy: 0.8202\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.3961 - accuracy: 0.8721 - val_loss: 0.2861 - val_accuracy: 0.9249\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 14s 46ms/step - loss: 0.2335 - accuracy: 0.9435 - val_loss: 0.2007 - val_accuracy: 0.9524\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 11s 36ms/step - loss: 0.1464 - accuracy: 0.9722 - val_loss: 0.1103 - val_accuracy: 0.9824\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.1390 - accuracy: 0.9737 - val_loss: 0.0907 - val_accuracy: 0.9862\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.0739 - accuracy: 0.9883 - val_loss: 0.0608 - val_accuracy: 0.9912\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 0.0548 - accuracy: 0.9915 - val_loss: 0.0461 - val_accuracy: 0.9929\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.0403 - accuracy: 0.9936 - val_loss: 0.0380 - val_accuracy: 0.9941\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 20s 65ms/step - loss: 0.0543 - accuracy: 0.9903 - val_loss: 0.0239 - val_accuracy: 0.9966\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 0.0207 - accuracy: 0.9968 - val_loss: 0.0171 - val_accuracy: 0.9976\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 20s 65ms/step - loss: 0.0608 - accuracy: 0.9889 - val_loss: 0.0602 - val_accuracy: 0.9890\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 0.0197 - accuracy: 0.9969 - val_loss: 0.0120 - val_accuracy: 0.9980\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 0.0109 - accuracy: 0.9983 - val_loss: 0.0109 - val_accuracy: 0.9981\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0079 - val_accuracy: 0.9987\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.0068 - val_accuracy: 0.9988\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 20s 65ms/step - loss: 0.0261 - accuracy: 0.9947 - val_loss: 0.0278 - val_accuracy: 0.9948\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 20s 64ms/step - loss: 0.0071 - accuracy: 0.9990 - val_loss: 0.0060 - val_accuracy: 0.9991\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.0036 - val_accuracy: 0.9994\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=n_epochs,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid),\n",
    "                    callbacks=[sampling_probability_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TFA seq2seq, the Keras subclassing API and attention mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTranslation(keras.models.Model):\n",
    "    def __init__(self, units=128, encoder_embedding_size=32,\n",
    "                 decoder_embedding_size=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder_embedding = keras.layers.Embedding(input_dim=len(INPUT_CHARS)+1,\n",
    "                                                       output_dim=encoder_embedding_size)\n",
    "        self.encoder_cell = keras.layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.decoder_embedding = keras.layers.Embedding(input_dim=len(OUTPUT_CHARS) + 2,\n",
    "                                                        output_dim=decoder_embedding_size)\n",
    "        self.attention = tfa.seq2seq.LuongAttention(units)\n",
    "        decoder_inner_cell = keras.layers.LSTMCell(units)\n",
    "        self.decoder_cell = tfa.seq2seq.AttentionWrapper(cell=decoder_inner_cell, attention_mechanism=self.attention)\n",
    "        output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(cell=self.decoder_cell, sampler=tfa.seq2seq.TrainingSampler(),\n",
    "                                               output_layer=output_layer)\n",
    "        self.inference_decoder=tfa.seq2seq.BasicDecoder(cell=self.decoder_cell, \n",
    "                                    sampler=tfa.seq2seq.GreedyEmbeddingSampler(embedding_fn=self.decoder_embedding),\n",
    "                                    output_layer=output_layer, maximum_iterations=max_output_length)\n",
    "    def call(self, inputs, training=None):\n",
    "        encoder_input, decoder_input = inputs\n",
    "        encoder_embeddings = self.encoder_embedding(encoder_input)\n",
    "        encoder_outputs, encoder_h, encoder_c = self.encoder_cell(encoder_embeddings, training=training)\n",
    "        encoder_state = [encoder_h, encoder_c]\n",
    "        self.attention(encoder_outputs, setup_memory=True)\n",
    "        decoder_embeddings = self.decoder_embedding(decoder_input)\n",
    "        decoder_initial_state = self.decoder_cell.get_initial_state(decoder_embeddings)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state = encoder_state)\n",
    "        if training:\n",
    "            decoder_outputs,_ ,_ = self.decoder(decoder_embeddings, \n",
    "                                                initial_state=decoder_initial_state, training=training)\n",
    "        else:\n",
    "            start_tokens = tf.zeros_like(encoder_input[:, 0])+sos_id\n",
    "            decoder_outputs, _, _ = self.inference_decoder(decoder_embeddings,\n",
    "                                            initial_state=decoder_initial_state, start_tokens=start_tokens, end_token=0)\n",
    "        return tf.nn.softmax(decoder_outputs.rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DateTranslation()\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "313/313 [==============================] - 23s 75ms/step - loss: 2.1298 - accuracy: 0.2349 - val_loss: 2.0020 - val_accuracy: 0.2772\n",
      "Epoch 2/25\n",
      "313/313 [==============================] - 26s 83ms/step - loss: 1.7828 - accuracy: 0.3580 - val_loss: 1.4823 - val_accuracy: 0.4426\n",
      "Epoch 3/25\n",
      "313/313 [==============================] - 25s 81ms/step - loss: 1.4399 - accuracy: 0.4754 - val_loss: 1.3733 - val_accuracy: 0.4854\n",
      "Epoch 4/25\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 1.3929 - accuracy: 0.4979 - val_loss: 1.3458 - val_accuracy: 0.4983\n",
      "Epoch 5/25\n",
      "313/313 [==============================] - 13s 43ms/step - loss: 1.1730 - accuracy: 0.5750 - val_loss: 0.9184 - val_accuracy: 0.6754\n",
      "Epoch 6/25\n",
      "313/313 [==============================] - 14s 44ms/step - loss: 0.7885 - accuracy: 0.7300 - val_loss: 0.4729 - val_accuracy: 0.8496\n",
      "Epoch 7/25\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 0.2604 - accuracy: 0.9270 - val_loss: 0.2213 - val_accuracy: 0.9621\n",
      "Epoch 8/25\n",
      "313/313 [==============================] - 26s 84ms/step - loss: 0.0865 - accuracy: 0.9880 - val_loss: 0.0340 - val_accuracy: 0.9983\n",
      "Epoch 9/25\n",
      "313/313 [==============================] - 26s 83ms/step - loss: 0.0227 - accuracy: 0.9992 - val_loss: 0.0167 - val_accuracy: 0.9997\n",
      "Epoch 10/25\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.0831 - accuracy: 0.9829 - val_loss: 0.6235 - val_accuracy: 0.8016\n",
      "Epoch 11/25\n",
      "313/313 [==============================] - 20s 63ms/step - loss: 0.2469 - accuracy: 0.9183 - val_loss: 0.1377 - val_accuracy: 0.9555\n",
      "Epoch 12/25\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.0577 - accuracy: 0.9843 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 13/25\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 14/25\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.0042 - accuracy: 0.9999 - val_loss: 0.0045 - val_accuracy: 0.9998\n",
      "Epoch 15/25\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.1433 - accuracy: 0.9692 - val_loss: 0.0467 - val_accuracy: 0.9853\n",
      "Epoch 16/25\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.0328 - accuracy: 0.9877 - val_loss: 0.0306 - val_accuracy: 0.9877\n",
      "Epoch 17/25\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.0287 - accuracy: 0.9881 - val_loss: 0.0292 - val_accuracy: 0.9873\n",
      "Epoch 18/25\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.0270 - accuracy: 0.9886 - val_loss: 0.0278 - val_accuracy: 0.9880\n",
      "Epoch 19/25\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.0147 - accuracy: 0.9930 - val_loss: 0.0105 - val_accuracy: 0.9946\n",
      "Epoch 20/25\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.0098 - accuracy: 0.9948 - val_loss: 0.0100 - val_accuracy: 0.9944\n",
      "Epoch 21/25\n",
      "313/313 [==============================] - 19s 61ms/step - loss: 0.0095 - accuracy: 0.9948 - val_loss: 0.0098 - val_accuracy: 0.9946\n",
      "Epoch 22/25\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.0094 - accuracy: 0.9949 - val_loss: 0.0101 - val_accuracy: 0.9944\n",
      "Epoch 23/25\n",
      "313/313 [==============================] - 21s 67ms/step - loss: 0.0093 - accuracy: 0.9948 - val_loss: 0.0095 - val_accuracy: 0.9944\n",
      "Epoch 24/25\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.0093 - accuracy: 0.9946 - val_loss: 0.0099 - val_accuracy: 0.9946\n",
      "Epoch 25/25\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.0093 - accuracy: 0.9947 - val_loss: 0.0094 - val_accuracy: 0.9946\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=25,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_predict_date_strs_v2(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    X_decoder = tf.zeros(shape=(len(X), max_output_length), dtype=tf.int32)\n",
    "    Y_probas = model.predict([X, X_decoder])\n",
    "    Y_pred = tf.argmax(Y_probas, axis=-1)\n",
    "    return ids_to_date_strs(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_predict_date_strs_v2([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
